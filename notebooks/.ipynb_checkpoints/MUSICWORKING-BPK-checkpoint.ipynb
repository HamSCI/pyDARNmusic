{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUSIC (MUltiple SIgnal Classification) Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MUSIC (MUltiple SIgnal Classification) algorithm is a rather generic and well-know algorithm in signal process used for the detection of multiple waves within an array of sensors [Schmidt, 1986].  MUSIC was originally used in SuperDARN by Samson et al. [1990] and Bristow et al. [1994] for the estimation of the characteristics of Medium Scale Traveling Ionospheric Disturbances (MSTIDs) detected by SuperDARN Radars.  These papers have inspired this implementation of the MUSIC algorithm.\n",
    "\n",
    "While this code has been written with SuperDARN detected MSTIDs in mind, it may be useful for working with other wave-like perturbations moving across the field of view of some geophysical instrument.\n",
    "\n",
    "This notebook demonstrates the use of the DaViTPy MUSIC module for MSTID parameter estimation.\n",
    "\n",
    "The DaViTPy MUSIC module is used by creating a musicArray object, which includes built-in methods for keeping track of all steps in the processing algorithm.  As the data is processed, all preceeding variants of the data are saved within the object and a history is created.  This makes it very easy to see what has been done with the data at any step along the way.\n",
    "\n",
    "*Written by N.A. Frissell, 4 November 2013.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "Bristow, W. A., R. A. Greenwald, and J. C. Samson (1994), Identification of high-latitude acoustic gravity wave sources using the Goose Bay HF Radar, J. Geophys. Res., 99(A1), 319–331, doi:10.1029/93JA01470.\n",
    "\n",
    "Samson, J. C., R. A. Greenwald, J. M. Ruohoniemi, A. Frey, and K. B. Baker (1990), Goose Bay radar observations of Earth-reflected, atmospheric gravity waves in the high-latitude ionosphere, J. Geophys. Res., 95(A6), 7693–7709, doi:10.1029/JA095iA06p07693.\n",
    "\n",
    "Schmidt, R.O., \"Multiple emitter location and signal parameter estimation,\" Antennas and Propagation, IEEE Transactions on, vol.34, no.3, pp.276,280, Mar 1986, doi:10.1109/TAP.1986.1143830."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMPORTANT: Please make sure to cite pyDARN in publications that use plots created by pyDARN using DOI: https://zenodo.org/record/3727269. Citing information for SuperDARN data is found at https://pydarn.readthedocs.io/en/master/user/citing/\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'musicRTI' from 'pyDARNmusic.musicPlot' (/home/fran/code/newMusic/MUSIC/pyDARNmusic/musicPlot.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyDARNmusic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmusicPlot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m musicRTI,musicFan,timeSeriesMultiPlot,plotRelativeRanges,spectrumMultiPlot,plotFullSpectrum,plotDlm, plotKarr,plotKarrDetected\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyDARNmusic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_fitacf\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyDARNmusic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m music\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'musicRTI' from 'pyDARNmusic.musicPlot' (/home/fran/code/newMusic/MUSIC/pyDARNmusic/musicPlot.py)"
     ]
    }
   ],
   "source": [
    "#Import the modules we need.\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "from pyDARNmusic.musicPlot import musicRTP,musicFan,timeSeriesMultiPlot,plotRelativeRanges,spectrumMultiPlot,plotFullSpectrum,plotDlm, plotKarr,plotKarrDetected\n",
    "from pyDARNmusic import load_fitacf\n",
    "from pyDARNmusic import music\n",
    "# from music import defineLimits, filterTimes,beamInterpolation,timeInterpolation,determineRelativePosition\n",
    "\n",
    "\n",
    "import pydarn\n",
    "import pydarnio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUSIC Processing Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Basic Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "radar   = 'bpk'\n",
    "sDate   = datetime.datetime(2017,1,15,1)\n",
    "eDate   = datetime.datetime(2017,1,15,23)\n",
    "fit_sfx = \"fitacf\"\n",
    "data_dir = f'/home/fran/code/SuperdarnW3usr/ForGitRepo/'\n",
    "fitacf  = load_fitacf(radar,sDate,eDate,data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(myPtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now create the data object.\n",
    "\n",
    "# By creating this object, data is taken from the SuperDARN database and rearranged into a numpy.array with\n",
    "# the shape (Nr Times, Nr Beams, Nr Gates).  Missing data is stored as np.nan.\n",
    "\n",
    "# By default, this command only loads in data where the ground scatter flag is True.\n",
    "\n",
    "# The fovModel='GS' indicates to calculate the field-of-view coordinates in using the ground-scatter mapping\n",
    "# formula.  This is standard when looking at MSTIDs using ground scatter.  See Bristow et al. [1994] for details.\n",
    "# dataObj_IS     = musicArray(myPtr,fovModel='IS')\n",
    "dataObj = music.musicArray(fitacf,sTime=sDate,eTime=eDate,fovModel='GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can list all of the data sets stored in this object.  Right now there is only one data set, but\n",
    "# as we go along, each step of processing will create a new data set.\n",
    "\n",
    "# Each data set is simply stored as an attribute of the dataObj, but the names of all of the data sets can be\n",
    "# accessed through the get_data_sets() method.\n",
    "dataObj.get_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Information about the data set is saved in it's metadata dictionary.  Some of the information in the metadata\n",
    "# dictionary is used to control certain plotting and processing variables in this module.\n",
    "\n",
    "dataObj.DS000_originalFit.printMetadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The music module also keeps track of each step of processing in using the data set's history attribute.\n",
    "\n",
    "dataObj.DS000_originalFit.printHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The 'active' attribute of dataObj is a reference to the most recently used data set of dataObj.  It is also\n",
    "# the default data set used by all DaViTPy MUSIC processing and plotting routines.  You can see this command\n",
    "# produces the same result as the cell above.\n",
    "\n",
    "dataObj.active.printHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can create an RTI plot and a fan plot from the data we loaded.\n",
    "# You can see that both range gate and geographic latitude are given on the y-axis, and the solar terminator\n",
    "# is also shaded in.  The terminator does not go below approximately range gate 10.  This is because the ground\n",
    "# scatter mapping formula is not defined for close-in range gates.\n",
    "\n",
    "fig = musicRTI(dataObj,beam=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can also make a fan plot.\n",
    "\n",
    "plotTime = datetime.datetime(2017,1,15,6,0)\n",
    "fig = musicFan(dataObj,time=plotTime)\n",
    "# import ipdb;ipdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We want to focus on just the data that contains the MSTID we are looking for, so we can apply some limits.\n",
    "\n",
    "# proc.music.defineLimits() does not actually modify the data, it just marks the data by putting an an entry\n",
    "# into the dataObj.active.metadata dictionary.  The limits will be applied later.  Right now, you can see\n",
    "# what data will be eliminated by making a new RTI plot.\n",
    "\n",
    "music.defineLimits(dataObj,gateLimits=[25,43])\n",
    "fig = musicRTI(dataObj,beam=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We also to restrict the amount of time we process.  Before actually running the music algorithm, we\n",
    "# will be filtering the data using a FIR filter that will eat up data at the beginning and end of the filter.\n",
    "# We can calculate exactly how much time that will be if we know some of the filter characteristics and\n",
    "# the time resolution of the data.  This can then be used to give us new start and end times.\n",
    "\n",
    "# For now, let's say we are going to use a filter with 101 taps and a dataset with 120 s resolution.\n",
    "numtaps = 101\n",
    "timeres = 120\n",
    "# numtaps = 8\n",
    "# timeres = 7\n",
    "\n",
    "#Let's also say that we are interested in the MSTID feature between 1400 and 1600 UT.\n",
    "sTime_of_interest = datetime.datetime(2017,1,15,4)\n",
    "eTime_of_interest = datetime.datetime(2017,1,15,6)\n",
    "\n",
    "#Now calculate the new start and end times...\n",
    "new_times = music.filterTimes(sTime_of_interest, eTime_of_interest, timeres, numtaps)\n",
    "music.defineLimits(dataObj,timeLimits=new_times)\n",
    "\n",
    "fig = musicRTI(dataObj,beam=13)\n",
    "# fig.savefig(\"/home/fran/Pictures/CEDAR2022/gateandtimelimitRTP.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we apply the limits and replot once more.\n",
    "# Note that many of the processing routines will automatically call applyLimits() before they\n",
    "# run the processing algorithm.\n",
    "# prob\n",
    "dataObj.active.applyLimits()\n",
    "fig = musicRTI(dataObj,beam=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Note that a new data set was created when we applied the limits.\n",
    "dataObj.get_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Also note that the history of the new object was updated.\n",
    "dataObj.active.printHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation and Cartesian Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Before we feed into the MUSIC Algorithm, we don't want our data to have any gaps in time or space.\n",
    "# Let's start by interpolating in space along the beams.\n",
    "music.beamInterpolation(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "proj= ccrs.PlateCarree()\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax  = fig.add_subplot(121, projection=proj,aspect='auto')\n",
    "musicFan(dataObj,plotZeros=True,dataSet='originalFit',axis=ax,time=plotTime)\n",
    "ax  = fig.add_subplot(122, projection=proj, aspect='auto')\n",
    "musicFan(dataObj,plotZeros=True,axis=ax,time=plotTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%debug\n",
    "# We also want to interpolate in time.  timeres=120 [seconds] was set in an earlier cell.\n",
    "music.timeInterpolation(dataObj,timeRes=timeres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "timeSeriesMultiPlot(dataObj,dataSet='timeInterpolated',dataSet2='beamInterpolated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We also want need to calculate a local cartesian grid for each cell we are going to use.\n",
    "music.determineRelativePosition(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#The black cell marks the center of the array.\n",
    "fig = plotRelativeRanges(dataObj,time=plotTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now filter the data.\n",
    "# numtaps=101 was set in an above cell.  The cutoff_frequencies are in Hz.\n",
    "filt = music.filter(dataObj, numtaps=numtaps, cutoff_low=0.0003, cutoff_high=0.0012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# At this point, the data has been filtered and saved to a new data set in dataObj.\n",
    "# Before we look at the data, let's look at the transfer function and impulse response of the data.\n",
    "\n",
    "fig = filt.plotTransferFunction(xmax=0.004)\n",
    "fig = filt.plotImpulseResponse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the filtered RTI plot.  We should set autoScale to True since the magnitudes will be much\n",
    "# lower than the original data.\n",
    "\n",
    "fig = musicRTI(dataObj,beam=15,autoScale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# You can see that the filter already marked off the data that you shouldn't use.\n",
    "# Just applyLimits() and replot.\n",
    "dataObj.active.applyLimits()\n",
    "fig = musicRTI(dataObj,beam=15,autoScale=True)\n",
    "# fig.savefig(\"/home/fran/Pictures/CEDAR2022/limitRTPintime.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Look at a fan plot.\n",
    "fig = musicFan(dataObj,time=plotTime,autoScale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We have now run all of the processing needed to feed the data into the spectral analysis and MUSIC.\n",
    "# Let's print the history just to recap what we have done.\n",
    "dataObj.active.printHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# First thing to do is to calculate the FFT of every cell...\n",
    "# from music import calculateFFT\n",
    "music.calculateFFT(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can look at the spectrum of select cells...\n",
    "# from plotting.musicPlot import spectrumMultiPlot\n",
    "spectrumMultiPlot(dataObj,xlim=(-0.0025,0.0025))\n",
    "spectrumMultiPlot(dataObj,plotType='magnitude',xlim=(0,0.0025))\n",
    "spectrumMultiPlot(dataObj,plotType='phase',xlim=(0,0.0025))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can also plot the full spectrum.  Here, every FFT bin contains 16 slices,\n",
    "# showing the data for each of the 16 radar beams from left to right.\n",
    "# Range gates are shown on the y-axis.\n",
    "# from plotting.musicPlot import plotFullSpectrum\n",
    "plotFullSpectrum(dataObj,xlim=(0,0.00175))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUSIC Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the cross-spectral matrix Dlm.\n",
    "# from music import calculateDlm\n",
    "music.calculateDlm(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can look at the Dlm matrix.  This usually isn't necessary for routine processing, but\n",
    "# it is good to know where the numbers are going...\n",
    "# from plotting.musicPlot import plotDlm\n",
    "plotDlm(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now, we finally run detect the horizontal wave numbers.\n",
    "# from music import calculateKarr\n",
    "music.calculateKarr(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can then plot the final result.\n",
    "# from plotting.musicPlot import plotKarr\n",
    "plotKarr(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Again, we can see all of the history gone into processing this plot.\n",
    "dataObj.active.printHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature/Signal Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We can use the image processing library in scikit-image to automatically find the peaks representing signals.\n",
    "# from music import detectSignals\n",
    "music.detectSignals(dataObj)\n",
    "plotKarr(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#All of the parameters connected with these signals is located in the sigDectect attribute.\n",
    "dataObj.active.sigDetect.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# It is possible to manually add a signal peak in.\n",
    "# All appropriate associated wave parameters will be calculated.\n",
    "# from music import add_signal\n",
    "music.add_signal(0.013,-0.015,dataObj)\n",
    "plotKarr(dataObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataObj.active.sigDetect.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Signals can also be removed from the database.\n",
    "# from music import del_signal\n",
    "music.del_signal(4,dataObj)\n",
    "plotKarr(dataObj)\n",
    "# fig.savefig(\"/home/fran/Pictures/CEDAR2022/bpkKRA.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# To get some insight on how the signal processing algorithm works, we can plot the detected groups.\n",
    "# You can adjust the threshold and neighborhood keywords on pydarn.proc.music.detectSignals() to tweak\n",
    "# the autodetection.\n",
    "# from plotting.musicPlot import plotKarrDetected\n",
    "plotKarrDetected(dataObj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfb1d0d5d45e308c1703e995971ca70df29b06cb3ef3c4914a8fa75bd8c261ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
